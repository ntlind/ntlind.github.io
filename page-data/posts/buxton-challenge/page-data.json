{"componentChunkName":"component---src-templates-post-template-js","path":"/posts/buxton-challenge/","result":{"data":{"markdownRemark":{"id":"8f9c947b-65db-5d17-93a8-909557f89441","html":"<p><img src=\"https://images.unsplash.com/photo-1486406146926-c627a92ad1ab?ixlib=rb-1.2.1&#x26;ixid=eyJhcHBfaWQiOjEyMDd9&#x26;auto=format&#x26;fit=crop&#x26;w=1050&#x26;q=80\" alt=\"Sean Pollock // Unsplash\"></p>\n<h1 id=\"summary\" style=\"position:relative;\"><a href=\"#summary\" aria-label=\"summary permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Summary</h1>\n<p>This write-up provides a high-level, technical overview of the approach that our team used to win the 2018 Buxton Challenge at the Wharton School. All model variables and results are captured in the final presentation.</p>\n<h2 id=\"data-import--partitioning\" style=\"position:relative;\"><a href=\"#data-import--partitioning\" aria-label=\"data import  partitioning permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Data Import &#x26; Partitioning</h2>\n<p>In this section, we began by importing all of the Subs by Tubbs data into R. We immediately split-off the observations with SALES_2017 = NULL and ensured that, outside of the 20 locations we were interested in forecasting for, there weren’t any observations with a blank response variable.</p>\n<h2 id=\"data-exploration\" style=\"position:relative;\"><a href=\"#data-exploration\" aria-label=\"data exploration permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Data Exploration</h2>\n<p>Next, we constructed plots and tables to explore:</p>\n<ul>\n<li>Which variables were either NULL or NA</li>\n<li>The distribution of our response variable, SALES_2017, which we found to be heavily right-skewed (suggesting the presence of extreme outliers)</li>\n<li>The average sales figures by STATE AND REGION (insights available in presentation deck)</li>\n<li>DENSITY_CLASS by STATE, where we found that, unsurprisingly, some states are more densely populated than others</li>\n<li>TRAFFIC_COUNT, where we noticed both a) missing values and b) extreme outliers</li>\n<li>University metrics, where we found a lack of contrast for most count and enrollment features</li>\n<li>Correlation tables to explore interactions between variables</li>\n</ul>\n<h2 id=\"data-cleaning\" style=\"position:relative;\"><a href=\"#data-cleaning\" aria-label=\"data cleaning permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Data Cleaning</h2>\n<p>In this section, we:</p>\n<ul>\n<li>\n<p>Dropped variables that we weren’t interested in using further:</p>\n<ul>\n<li>University variables that lacked contrast</li>\n<li>YEAR_OPEN, since this variable wasn’t meaningful for predictive purposes</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"feature-engineering--data-transformation\" style=\"position:relative;\"><a href=\"#feature-engineering--data-transformation\" aria-label=\"feature engineering  data transformation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Feature Engineering &#x26; Data Transformation</h2>\n<p>We tested many different features throughout our modeling process, including</p>\n<ul>\n<li>Log-transforming SALES and our numeric x-variables in an attempt to mitigate the effect of outliers (decreased out-of-sample [OOS] fit)</li>\n<li>Adding polynomial interaction terms (not found to be significant)</li>\n<li>Finding the average sales per customer by dividing SALES<em>2017/TRAFFIC</em>COUNT</li>\n<li>Removing sales outliers that were outside of 3 standard deviations from our mean (though we thought we’d be unlikely to account for these strong deviances given our feature set, this variable actually hurt OOS fit and was removed)</li>\n<li>Standardizing all numeric features so that there mean was 0 and standard deviation was 1 (hurt OOS fit, so was turned-off in our final model)</li>\n<li>Filling in TRAFFIC<em>COUNT using a simple linear model of other variables; indicator was also generated to indicate absence of TRAFFIC</em>COUNT</li>\n<li>Appended Principal Component features to the dataset in an effort to improve the fit of gradient-boosted models</li>\n</ul>\n<h2 id=\"dimensionality-reduction\" style=\"position:relative;\"><a href=\"#dimensionality-reduction\" aria-label=\"dimensionality reduction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Dimensionality Reduction</h2>\n<p>After creating all of these new and interesting features, we needed a way to efficiently choose the most important variables out of our 1200+ features. We relied on two approaches:</p>\n<ul>\n<li>Taking subsets of features using business intuition</li>\n<li>Using 10-fold, cross-validated ElasticNet on a subsample of the data to identify a subset of features that are likely to be predictive, then throwing away variables based on a) business intuition and b) statistical significance</li>\n</ul>\n<p>To get the best results from ElasticNet, we first ran a cross-validated model to select the best α parameter (chooses between ridge and lasso regression) by minimizing our out-of-sample error metric. We then ran another 10-fold model with that α parameter to find the best λ value (regularization parameter) in order to find our error-minimizing set of variables. We chose to use <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi><mi mathvariant=\"normal\">.</mi><mi>m</mi><mi>i</mi><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">lambda.min</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">a</span><span class=\"mord mathnormal\">m</span><span class=\"mord mathnormal\">b</span><span class=\"mord mathnormal\">d</span><span class=\"mord mathnormal\">a</span><span class=\"mord\">.</span><span class=\"mord mathnormal\">m</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">n</span></span></span></span> instead of  λ.1se to get a more conservative (read: larger) set of variables that could then be refined manually.</p>\n<p>After this process was completed, we were left with two different sets of formulae that could be used in our modeling procedures:</p>\n<ul>\n<li><strong>EN_output</strong>: the 100-or-so variables that came directly out of our 10-fold ElasticNet</li>\n<li><strong>lm_input</strong>: The finalized set of statistically-significant variables as per our final linear regression</li>\n</ul>\n<h2 id=\"interpreting-our-linear-model\" style=\"position:relative;\"><a href=\"#interpreting-our-linear-model\" aria-label=\"interpreting our linear model permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Interpreting Our Linear Model</h2>\n<p>By running our linear model with lm_input, we created the regression summary shown in the final presentation. We took care to explore our regression’s diagnostic plots, where we found two notable observations:</p>\n<ul>\n<li>Our residuals were approximately normally distributed, which is what we hope for</li>\n<li>We noticed some flare at either end of the spectrum as our model struggled to accurately predict sales for heavy outliers. This suggests that some of our interpretations will not hold for extreme outliers</li>\n<li>Little to no visible heteroskedasticity in our data</li>\n</ul>\n<h2 id=\"forecasting-loop--final-predictions\" style=\"position:relative;\"><a href=\"#forecasting-loop--final-predictions\" aria-label=\"forecasting loop  final predictions permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Forecasting Loop &#x26; Final Predictions</h2>\n<p>In this section, our goal was to quickly and efficiently test our 13 different models to determine which fit would provide us with the best out-of-sample results (defined as minimizing mean average percent error). The models we tested include:</p>\n<ul>\n<li>Two linear regression on both EN<em>output and lm</em>input</li>\n<li>Bayesian regression (stabilized using Cauchy distribution)</li>\n<li>XGBoost (a gradient-boosted algorithm designed to discover the Higgs boson particle) on all variables</li>\n<li>Two SVMs with linear kernels, one on lm<em>input and one on EN</em>output</li>\n<li>Two SVMs with polynomial kernels, one on lm<em>input and one on EN</em>output</li>\n<li>Two Gradient Boosted Machines, one only on the EN_output variables</li>\n<li>One Random Forest</li>\n</ul>\n<p>Before running our actual forecasting loop, we tuned our tree-based algorithms separately using 10-fold cross-validation to gridsearch over a wide range of parameters. The tuned parameters were then fed directly into each respective algorithm during the forecasting loop.</p>\n<p>The forecasting loop itself did the following:</p>\n<ul>\n<li>Automatically apply log transformations and scaling if designated in parameter control (turned off in the above code)</li>\n<li>\n<p>For each subsample of data (drawn using Monte Carlo cross-validation):</p>\n<ul>\n<li>Split the data into test and training sets</li>\n<li>Find Principal Components for training set if turned on and predict PCs for test set</li>\n<li>Run each model</li>\n<li>Capture in-sample and out-of-sample predictions</li>\n<li>Report all metrics in a nice, easy to read dataframe</li>\n</ul>\n</li>\n</ul>\n<p>In our loop output, we’d generally notice:</p>\n<ul>\n<li>Linear regression performs relatively well across iterations of the loop. Occasionally, our model will overtrain on outliers and put out forecasts that don’t closely match the out-of-sample data</li>\n<li>XGBoost always performs extremely well and generally has less variance in its error terms than other models</li>\n<li>Random Forest also performs well, but tends to overfit on specific outliers</li>\n<li>SVM varies wildly between iterations</li>\n<li>Other models do not perform poorly, but do not ever catch up lm() or XGBoost in performance</li>\n</ul>\n<p>This loop enabled our team to constantly tweak our transformations and feature sets over time while observing the effect that our actions were having on our out-of-sample error metrics. After running many different iterations of our modeling loop, we chose to ensemble our XGBoost, linear regression, and RF outputs (without scaling or log transformations) by taking a simple average of all in order to arrive at our final forecasts. </p>","fields":{"slug":"/posts/buxton-challenge/","tagSlugs":["/tag/data-science/","/tag/competition/","/tag/project-write-up/"]},"frontmatter":{"date":"2018-02-01T23:46:37.121Z","description":"A brief overview of the technical steps used to win the 2018 Buxton Challenge at the Wharton School.","tags":["data science","competition","project write-up"],"title":"Winning the Buxton Challenge"}}},"pageContext":{"slug":"/posts/buxton-challenge/"}},"staticQueryHashes":["251939775","3613830147","401334301"]}